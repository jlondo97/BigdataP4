{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de sentimientos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Importaciones de bibliotecas y librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hdpjupyter.dis.eafit.edu.co:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2.3.1.4.0-315</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=pyspark-shell>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "import numpy\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.functions import length\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesta de datos de hdsf en dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+--------------+--------------------+----------+------+-----+----+--------------------+\n",
      "| _c0|  _c1|                 _c2|           _c3|                 _c4|       _c5|   _c6|  _c7| _c8|                 _c9|\n",
      "+----+-----+--------------------+--------------+--------------------+----------+------+-----+----+--------------------+\n",
      "|null|   id|               title|   publication|              author|      date|  year|month| url|             content|\n",
      "|   0|17283|House Republicans...|New York Times|          Carl Hulse|2016-12-31|2016.0| 12.0|null|WASHINGTON  —   C...|\n",
      "|   1|17284|Rift Between Offi...|New York Times|Benjamin Mueller ...|2017-06-19|2017.0|  6.0|null|After the bullet ...|\n",
      "|   2|17285|Tyrus Wong, ‘Bamb...|New York Times|        Margalit Fox|2017-01-06|2017.0|  1.0|null|When Walt Disney’...|\n",
      "|   3|17286|Among Deaths in 2...|New York Times|    William McDonald|2017-04-10|2017.0|  4.0|null|Death may be the ...|\n",
      "|   4|17287|Kim Jong-un Says ...|New York Times|       Choe Sang-Hun|2017-01-02|2017.0|  1.0|null|SEOUL, South Kore...|\n",
      "|   5|17288|Sick With a Cold,...|New York Times|         Sewell Chan|2017-01-02|2017.0|  1.0|null|LONDON  —   Queen...|\n",
      "|   6|17289|Taiwan’s Presiden...|New York Times| Javier C. Hernández|2017-01-02|2017.0|  1.0|null|BEIJING  —   Pres...|\n",
      "|   7|17290|After ‘The Bigges...|New York Times|         Gina Kolata|2017-02-08|2017.0|  2.0|null|Danny Cahill stoo...|\n",
      "|   8|17291|First, a Mixtape....|New York Times|    Katherine Rosman|2016-12-31|2016.0| 12.0|null|Just how   is Hil...|\n",
      "|   9|17292|Calling on Angels...|New York Times|         Andy Newman|2016-12-31|2016.0| 12.0|null|Angels are everyw...|\n",
      "|  10|17293|Weak Federal Powe...|New York Times|       Justin Gillis|2017-01-03|2017.0|  1.0|null|With Donald J. Tr...|\n",
      "|  11|17294|Can Carbon Captur...|New York Times|       John Schwartz|2017-01-05|2017.0|  1.0|null|THOMPSONS, Tex.  ...|\n",
      "|  12|17295|Mar-a-Lago, the F...|New York Times|     Maggie Haberman|2017-01-02|2017.0|  1.0|null|WEST PALM BEACH, ...|\n",
      "|  13|17296|How to form healt...|New York Times|      Charles Duhigg|2017-01-02|2017.0|  1.0|null|This article is p...|\n",
      "|  14|17297|Turning Your Vaca...|New York Times|Stephanie Rosenbloom|2017-04-14|2017.0|  4.0|null|It’s the season f...|\n",
      "|  15|17298|As Second Avenue ...|New York Times| Emma G. Fitzsimmons|2017-01-02|2017.0|  1.0|null|Finally. The Seco...|\n",
      "|  16|17300|Dylann Roof Himse...|New York Times|Kevin Sack and Al...|2017-01-02|2017.0|  1.0|null|  pages into the ...|\n",
      "|  17|17301|Modi’s Cash Ban B...|New York Times|         Geeta Anand|2017-01-02|2017.0|  1.0|null|MUMBAI, India  — ...|\n",
      "|  18|17302|Suicide Bombing i...|New York Times|The Associated Press|2017-01-03|2017.0|  1.0|null|BAGHDAD  —   A su...|\n",
      "+----+-----+--------------------+--------------+--------------------+----------+------+-----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv = '/user/jlondo97/datasets/articles1.csv'\n",
    "df1 = spark.read.csv(csv)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definicion de expresiones regulares para la limpieza de los contenidos de las diferentes publicaciones \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = '[^a-zA-Z ]'\n",
    "reg1 = '[\\s*]{1,}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza del DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creando un dataframe que contenga los contedidos de las publicaciones hechas y limpiando el contenido de caracteres especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 _c9|               clean|\n",
      "+--------------------+--------------------+\n",
      "|             content|             content|\n",
      "|WASHINGTON  —   C...|WASHINGTON     Co...|\n",
      "|After the bullet ...|After the bullet ...|\n",
      "|When Walt Disney’...|When Walt Disneys...|\n",
      "|Death may be the ...|Death may be the ...|\n",
      "|SEOUL, South Kore...|SEOUL South Korea...|\n",
      "|LONDON  —   Queen...|LONDON     Queen ...|\n",
      "|BEIJING  —   Pres...|BEIJING     Presi...|\n",
      "|Danny Cahill stoo...|Danny Cahill stoo...|\n",
      "|Just how   is Hil...|Just how   is Hil...|\n",
      "|Angels are everyw...|Angels are everyw...|\n",
      "|With Donald J. Tr...|With Donald J Tru...|\n",
      "|THOMPSONS, Tex.  ...|THOMPSONS Tex    ...|\n",
      "|WEST PALM BEACH, ...|WEST PALM BEACH F...|\n",
      "|This article is p...|This article is p...|\n",
      "|It’s the season f...|Its the season fo...|\n",
      "|Finally. The Seco...|Finally The Secon...|\n",
      "|  pages into the ...|  pages into the ...|\n",
      "|MUMBAI, India  — ...|MUMBAI India     ...|\n",
      "|BAGHDAD  —   A su...|BAGHDAD     A sui...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = df1.withColumn(\"clean\", regexp_replace('_c9', reg ,\"\"))\n",
    "df_1.select('_c9','clean').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las siguientes lineas se eliminan las espacios en blanco(\\s) de mas que son tokenisados como tokens independientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               clean|              clean1|\n",
      "+--------------------+--------------------+\n",
      "|             content|             content|\n",
      "|WASHINGTON     Co...|WASHINGTON Congre...|\n",
      "|After the bullet ...|After the bullet ...|\n",
      "|When Walt Disneys...|When Walt Disneys...|\n",
      "|Death may be the ...|Death may be the ...|\n",
      "|SEOUL South Korea...|SEOUL South Korea...|\n",
      "|LONDON     Queen ...|LONDON Queen Eliz...|\n",
      "|BEIJING     Presi...|BEIJING President...|\n",
      "|Danny Cahill stoo...|Danny Cahill stoo...|\n",
      "|Just how   is Hil...|Just how is Hilla...|\n",
      "|Angels are everyw...|Angels are everyw...|\n",
      "|With Donald J Tru...|With Donald J Tru...|\n",
      "|THOMPSONS Tex    ...|THOMPSONS Tex Can...|\n",
      "|WEST PALM BEACH F...|WEST PALM BEACH F...|\n",
      "|This article is p...|This article is p...|\n",
      "|Its the season fo...|Its the season fo...|\n",
      "|Finally The Secon...|Finally The Secon...|\n",
      "|  pages into the ...| pages into the j...|\n",
      "|MUMBAI India     ...|MUMBAI India It w...|\n",
      "|BAGHDAD     A sui...|BAGHDAD A suicide...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = df_1.withColumn(\"clean1\", regexp_replace('clean', reg1 ,\" \"))\n",
    "df_2.select('clean','clean1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización de los contenidos de las publicaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creacion de un dataframe con el contenido de la publicacion tokenizado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df=tokenization.transform(df_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization=Tokenizer(inputCol='clean1',outputCol='tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              clean1|              tokens|\n",
      "+--------------------+--------------------+\n",
      "|             content|           [content]|\n",
      "|WASHINGTON Congre...|[washington, cong...|\n",
      "|After the bullet ...|[after, the, bull...|\n",
      "|When Walt Disneys...|[when, walt, disn...|\n",
      "|Death may be the ...|[death, may, be, ...|\n",
      "|SEOUL South Korea...|[seoul, south, ko...|\n",
      "|LONDON Queen Eliz...|[london, queen, e...|\n",
      "|BEIJING President...|[beijing, preside...|\n",
      "|Danny Cahill stoo...|[danny, cahill, s...|\n",
      "|Just how is Hilla...|[just, how, is, h...|\n",
      "|Angels are everyw...|[angels, are, eve...|\n",
      "|With Donald J Tru...|[with, donald, j,...|\n",
      "|THOMPSONS Tex Can...|[thompsons, tex, ...|\n",
      "|WEST PALM BEACH F...|[west, palm, beac...|\n",
      "|This article is p...|[this, article, i...|\n",
      "|Its the season fo...|[its, the, season...|\n",
      "|Finally The Secon...|[finally, the, se...|\n",
      "| pages into the j...|[, pages, into, t...|\n",
      "|MUMBAI India It w...|[mumbai, india, i...|\n",
      "|BAGHDAD A suicide...|[baghdad, a, suic...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_df.select('clean1','tokens').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remoción de stopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminacion de stopWord de los contenidos de las publicaciones token tales como \"I, and .or\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_df=stopword_removal.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              clean1|              tokens|      refined_tokens|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|             content|           [content]|           [content]|\n",
      "|WASHINGTON Congre...|[washington, cong...|[washington, cong...|\n",
      "|After the bullet ...|[after, the, bull...|[bullet, shells, ...|\n",
      "|When Walt Disneys...|[when, walt, disn...|[walt, disneys, b...|\n",
      "|Death may be the ...|[death, may, be, ...|[death, may, grea...|\n",
      "|SEOUL South Korea...|[seoul, south, ko...|[seoul, south, ko...|\n",
      "|LONDON Queen Eliz...|[london, queen, e...|[london, queen, e...|\n",
      "|BEIJING President...|[beijing, preside...|[beijing, preside...|\n",
      "|Danny Cahill stoo...|[danny, cahill, s...|[danny, cahill, s...|\n",
      "|Just how is Hilla...|[just, how, is, h...|[hillary, kerr, f...|\n",
      "|Angels are everyw...|[angels, are, eve...|[angels, everywhe...|\n",
      "|With Donald J Tru...|[with, donald, j,...|[donald, j, trump...|\n",
      "|THOMPSONS Tex Can...|[thompsons, tex, ...|[thompsons, tex, ...|\n",
      "|WEST PALM BEACH F...|[west, palm, beac...|[west, palm, beac...|\n",
      "|This article is p...|[this, article, i...|[article, part, s...|\n",
      "|Its the season fo...|[its, the, season...|[season, family, ...|\n",
      "|Finally The Secon...|[finally, the, se...|[finally, second,...|\n",
      "| pages into the j...|[, pages, into, t...|[, pages, journal...|\n",
      "|MUMBAI India It w...|[mumbai, india, i...|[mumbai, india, b...|\n",
      "|BAGHDAD A suicide...|[baghdad, a, suic...|[baghdad, suicide...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_df.select('clean1','tokens','refined_tokens').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorizando "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, clean: string, clean1: string, tokens: array<string>, refined_tokens: array<string>]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_udf = udf(lambda s: len(s), IntegerType())\n",
    "refined_df = refined_df.withColumn(\"token_count\", len_udf(col('refined_tokens')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------+\n",
      "|              clean1|              tokens|      refined_tokens|token_count|\n",
      "+--------------------+--------------------+--------------------+-----------+\n",
      "|             content|           [content]|           [content]|          1|\n",
      "|WASHINGTON Congre...|[washington, cong...|[washington, cong...|        493|\n",
      "|After the bullet ...|[after, the, bull...|[bullet, shells, ...|       2553|\n",
      "|When Walt Disneys...|[when, walt, disn...|[walt, disneys, b...|       1286|\n",
      "|Death may be the ...|[death, may, be, ...|[death, may, grea...|       1182|\n",
      "|SEOUL South Korea...|[seoul, south, ko...|[seoul, south, ko...|        418|\n",
      "|LONDON Queen Eliz...|[london, queen, e...|[london, queen, e...|         86|\n",
      "|BEIJING President...|[beijing, preside...|[beijing, preside...|        345|\n",
      "|Danny Cahill stoo...|[danny, cahill, s...|[danny, cahill, s...|       1618|\n",
      "|Just how is Hilla...|[just, how, is, h...|[hillary, kerr, f...|        879|\n",
      "|Angels are everyw...|[angels, are, eve...|[angels, everywhe...|        424|\n",
      "|With Donald J Tru...|[with, donald, j,...|[donald, j, trump...|        639|\n",
      "|THOMPSONS Tex Can...|[thompsons, tex, ...|[thompsons, tex, ...|       1021|\n",
      "|WEST PALM BEACH F...|[west, palm, beac...|[west, palm, beac...|        769|\n",
      "|This article is p...|[this, article, i...|[article, part, s...|        327|\n",
      "|Its the season fo...|[its, the, season...|[season, family, ...|        936|\n",
      "|Finally The Secon...|[finally, the, se...|[finally, second,...|        760|\n",
      "| pages into the j...|[, pages, into, t...|[, pages, journal...|        881|\n",
      "|MUMBAI India It w...|[mumbai, india, i...|[mumbai, india, b...|        857|\n",
      "|BAGHDAD A suicide...|[baghdad, a, suic...|[baghdad, suicide...|        329|\n",
      "+--------------------+--------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_df.select('clean1','tokens','refined_tokens','token_count').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
