{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesta de datos de hdsf en dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = '/user/jlondo97/datasets/articles1.csv'\n",
    "df1 = spark.read.csv(csv,inferSchema=True,header=True)\n",
    "csv = '/user/jlondo97/datasets/articles2.csv'\n",
    "df2 = spark.read.csv(csv,inferSchema=True,header=True)\n",
    "csv = '/user/jlondo97/datasets/articles3.csv'\n",
    "df3 = spark.read.csv(csv,inferSchema=True,header=True)\n",
    "# df1.show()\n",
    "# df2.show()\n",
    "# df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------------------+--------------+--------------------+----------+------+-----+----+--------------------+\n",
      "|  _c0|   id|               title|   publication|              author|      date|  year|month| url|             content|\n",
      "+-----+-----+--------------------+--------------+--------------------+----------+------+-----+----+--------------------+\n",
      "|10092|28828|Watch: Amazon Bos...|     Breitbart|         Nate Church|2017-03-21|2017.0|  3.0|null|At the MARS 2017 ...|\n",
      "|10101|28837|Patriots Owner Ro...|     Breitbart|         Trent Baker|2017-02-03|2017.0|  2.0|null|”I remember who t...|\n",
      "|10236|28972|Report: George So...|     Breitbart|         Aaron Klein|2017-01-23|2017.0|  1.0|null|Billionaire Georg...|\n",
      "|10513|29249|Peter Schweizer: ...|     Breitbart|        John Hayward|2017-05-26|2017.0|  5.0|null|On Friday’s Breit...|\n",
      "|10608|29344|Mexican Border St...|     Breitbart|   Cartel Chronicles|2017-02-19|2017.0|  2.0|null|PIEDRAS NEGRAS, C...|\n",
      "|10646|29382|Maxine Waters: ’D...|     Breitbart|             Pam Key|2017-05-14|2017.0|  5.0|null|Saturday at a tow...|\n",
      "|  109|17410|Putin Led a Compl...|New York Times|Michael D. Shear ...|2017-01-07|2017.0|  1.0|null|WASHINGTON  —   P...|\n",
      "|11015|29751|White House: ’No ...|     Breitbart|          Edwin Mora|2017-01-25|2017.0|  1.0|null|White House press...|\n",
      "|11085|29821|Biggest Insurance...|     Breitbart|          Sean Moran|2017-03-10|2017.0|  3.0|null|Anthem, the natio...|\n",
      "|11268|30004|Miller: We Have P...|     Breitbart|             Pam Key|2017-02-12|2017.0|  2.0|null|Sunday on ABC’s “...|\n",
      "|11302|30038|Speaker Ryan’s Ob...|     Breitbart|          Sean Moran|2017-03-10|2017.0|  3.0|null|Speaker Ryan’s pl...|\n",
      "|11331|30067|Ann Coulter Tweet...|     Breitbart|         Lucas Nolan|2017-05-13|2017.0|  5.0|null|Conservative fire...|\n",
      "|11599|30335|Conway to Chuck T...|     Breitbart|             Pam Key|2017-01-22|2017.0|  1.0|null|Sunday on NBC’s “...|\n",
      "|11695|30431|WATCH: Vandals Tw...|     Breitbart|       Deborah Danan|2017-01-01|2017.0|  1.0|null|TEL AVIV  —   A J...|\n",
      "|11732|30468|Politico Magazine...|     Breitbart|         AWR Hawkins|2017-04-30|2017.0|  4.0|null|Although the NRA ...|\n",
      "|11766|30502|HHS Sec Price: Ex...|     Breitbart|           Jeff Poor|2017-05-12|2017.0|  5.0|null|Friday on Hugh He...|\n",
      "|11961|30697| Professional Act...|     Breitbart|         Aaron Klein|2017-01-18|2017.0|  1.0|null|While the protest...|\n",
      "|12115|30851|Putin: Syria Chem...|     Breitbart|        John Hayward|2017-04-11|2017.0|  4.0|null|At a Tuesday pres...|\n",
      "|12806|31542|Former Anthem-Pro...|     Breitbart|         Dylan Gwinn|2017-04-16|2017.0|  4.0|null|Players that prot...|\n",
      "|12830|31566|Teen Vogue Calls ...|     Breitbart|       Jerome Hudson|2017-06-12|2017.0|  6.0|null|  magazine Teen V...|\n",
      "+-----+-----+--------------------+--------------+--------------------+----------+------+-----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_1_df_2 = df1.join(df2, on=['_c0', 'id', 'title', 'publication', 'author', 'date', 'year', 'month', 'url', 'content'], how='left_outer')\n",
    "full_df = join_1_df_2.join(df3, on=['_c0', 'id', 'title', 'publication', 'author', 'date', 'year', 'month', 'url', 'content'], how='left_outer')\n",
    "full_df = full_df.limit(100)\n",
    "full_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza del DataFrame\n",
    "Creando un dataframe que contenga los contedidos de las publicaciones hechas y limpiando el contenido de caracteres especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = '[^a-zA-Z ]'\n",
    "reg1 = '[\\s*]{1,}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_copy = full_df.withColumn(\"ltrimmed_word\",ltrim(col(\"Content\")))\n",
    "full_df = full_copy.withColumn(\"clean\", regexp_replace('ltrimmed_word', reg ,\"\"))\n",
    "full_df.select('ltrimmed_word','clean').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_df = full_df.withColumn(\"clean1\", regexp_replace('clean', reg1 ,\" \"))\n",
    "full_df.select('clean','clean1').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización de los contenidos de las publicaciones\n",
    "Creacion de un dataframe con el contenido de la publicacion tokenizado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization=Tokenizer(inputCol='clean1',outputCol='tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df = tokenization.transform(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_df.select('clean1','tokens').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminar stopWords\n",
    "Eliminación de stopWord en la columna de contenido de las publicaciones, token tales como \"I, and .or\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_df=stopword_removal.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "refined_df.select('clean1','tokens','refined_tokens').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización del DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_udf = udf(lambda s: len(s), IntegerType())\n",
    "refined_df = refined_df.withColumn(\"token_count\", len_udf(col('refined_tokens')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "refined_df.select('clean1','tokens','refined_tokens','token_count').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupación de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, HashingTF, IDF\n",
    "from pyspark.ml.clustering import LDA, KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aux_df = refined_df\n",
    "aux_df = aux_df.drop('publication', 'author', 'publication', 'content', 'date', 'year', 'month', 'url', 'clean', 'clean1', 'tokens')\n",
    "aux_df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill = array().cast(\"array<string>\")\n",
    "tokens_a = when(col(\"refined_tokens\").isNull(), fill).otherwise(col(\"refined_tokens\"))\n",
    "aux_df = aux_df.withColumn(\"refined_tokens\", tokens_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(numFeatures=2000, inputCol=\"refined_tokens\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)\n",
    "kmeans = KMeans(k=25)\n",
    "# lda = LDA(k=25, seed=123, optimizer=\"em\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages=[hashingTF, idf, kmeans])\n",
    "# pipeline = Pipeline(stages=[hashingTF, idf, kmeans, lda])\n",
    "model = pipeline.fit(aux_df)\n",
    "results = model.transform(aux_df)\n",
    "results.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.select('refined_tokens', 'features', 'prediction').show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupBy('prediction').count().show(100)\n",
    "# results.withColumn(\"aux\", print_columns(\"refined_tokens\")) # .select(\"aux\").show()\n",
    "# results.filter(\"prediction = 13\").select('refined_tokens', 'features', 'prediction').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(inputCol=\"refined_tokens\", outputCol=\"rawFeatures\", vocabSize = 900)\n",
    "# cvmodel = cv.fit(aux_df)\n",
    "# featurizedData = cvmodel.transform(aux_df)\n",
    "# vocab = cvmodel.vocabulary\n",
    "# vocab_broadcast = sc.broadcast(vocab)\n",
    "# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "# idfModel = idf.fit(featurizedData)\n",
    "# rescaledData = idfModel.transform(featurizedData)\n",
    "# rescaledData.select(\"refined_tokens\", \"features\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda = LDA(k=25, seed=100, optimizer=\"em\", featuresCol=\"features\")\n",
    "# ldamodel = lda.fit(rescaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ldamodel.isDistributed()\n",
    "# ldamodel.vocabSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldatopics = ldamodel.describeTopics()\n",
    "# ldatopics.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.select('refined_tokens', 'features', 'prediction').show(100, truncate = True) #.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
