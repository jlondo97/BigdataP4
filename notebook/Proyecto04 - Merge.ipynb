{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "import numpy\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.functions import length\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesta de datos de hdsf en dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = '/user/jlondo97/datasets/articles1.csv'\n",
    "df1 = spark.read.csv(csv)\n",
    "csv = '/user/jlondo97/datasets/articles2.csv'\n",
    "df2 = spark.read.csv(csv)\n",
    "csv = '/user/jlondo97/datasets/articles3.csv'\n",
    "df3 = spark.read.csv(csv)\n",
    "# df1.show()\n",
    "# df2.show()\n",
    "# df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------+--------------------+----------------+------+-----+----+--------------------+\n",
      "|   ID|               Title|   Publication|              Author|Publication_date|  Year|Month| URL|             Content|\n",
      "+-----+--------------------+--------------+--------------------+----------------+------+-----+----+--------------------+\n",
      "|28828|Watch: Amazon Bos...|     Breitbart|         Nate Church|      2017-03-21|2017.0|  3.0|null|At the MARS 2017 ...|\n",
      "|28837|Patriots Owner Ro...|     Breitbart|         Trent Baker|      2017-02-03|2017.0|  2.0|null|”I remember who t...|\n",
      "|28972|Report: George So...|     Breitbart|         Aaron Klein|      2017-01-23|2017.0|  1.0|null|Billionaire Georg...|\n",
      "|29249|Peter Schweizer: ...|     Breitbart|        John Hayward|      2017-05-26|2017.0|  5.0|null|On Friday’s Breit...|\n",
      "|29344|Mexican Border St...|     Breitbart|   Cartel Chronicles|      2017-02-19|2017.0|  2.0|null|PIEDRAS NEGRAS, C...|\n",
      "|29382|Maxine Waters: ’D...|     Breitbart|             Pam Key|      2017-05-14|2017.0|  5.0|null|Saturday at a tow...|\n",
      "|17410|Putin Led a Compl...|New York Times|Michael D. Shear ...|      2017-01-07|2017.0|  1.0|null|WASHINGTON  —   P...|\n",
      "|29751|White House: ’No ...|     Breitbart|          Edwin Mora|      2017-01-25|2017.0|  1.0|null|White House press...|\n",
      "|29821|Biggest Insurance...|     Breitbart|          Sean Moran|      2017-03-10|2017.0|  3.0|null|Anthem, the natio...|\n",
      "|30004|Miller: We Have P...|     Breitbart|             Pam Key|      2017-02-12|2017.0|  2.0|null|Sunday on ABC’s “...|\n",
      "|30038|Speaker Ryan’s Ob...|     Breitbart|          Sean Moran|      2017-03-10|2017.0|  3.0|null|Speaker Ryan’s pl...|\n",
      "|30067|Ann Coulter Tweet...|     Breitbart|         Lucas Nolan|      2017-05-13|2017.0|  5.0|null|Conservative fire...|\n",
      "|30335|Conway to Chuck T...|     Breitbart|             Pam Key|      2017-01-22|2017.0|  1.0|null|Sunday on NBC’s “...|\n",
      "|30431|WATCH: Vandals Tw...|     Breitbart|       Deborah Danan|      2017-01-01|2017.0|  1.0|null|TEL AVIV  —   A J...|\n",
      "|30468|Politico Magazine...|     Breitbart|         AWR Hawkins|      2017-04-30|2017.0|  4.0|null|Although the NRA ...|\n",
      "|30502|HHS Sec Price: Ex...|     Breitbart|           Jeff Poor|      2017-05-12|2017.0|  5.0|null|Friday on Hugh He...|\n",
      "|30697| Professional Act...|     Breitbart|         Aaron Klein|      2017-01-18|2017.0|  1.0|null|While the protest...|\n",
      "|30851|Putin: Syria Chem...|     Breitbart|        John Hayward|      2017-04-11|2017.0|  4.0|null|At a Tuesday pres...|\n",
      "|31542|Former Anthem-Pro...|     Breitbart|         Dylan Gwinn|      2017-04-16|2017.0|  4.0|null|Players that prot...|\n",
      "|31566|Teen Vogue Calls ...|     Breitbart|       Jerome Hudson|      2017-06-12|2017.0|  6.0|null|  magazine Teen V...|\n",
      "+-----+--------------------+--------------+--------------------+----------------+------+-----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_1_df_2 = df1.join(df2, on=['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9'], how='left_outer')\n",
    "full_df = join_1_df_2.join(df3, on=['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9'], how='left_outer')\n",
    "full_df = full_df.selectExpr(\"_c1 as ID\", \"_c2 as Title\", \"_c3 as Publication\", \"_c4 as Author\", \"_c5 as Publication_date\", \"_c6 as Year\", \"_c7 as Month\", \"_c8 as URL\", \"_c9 as Content\")\n",
    "full_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza del DataFrame\n",
    "Creando un dataframe que contenga los contedidos de las publicaciones hechas y limpiando el contenido de caracteres especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = '[^a-zA-Z ]'\n",
    "reg1 = '[\\s*]{1,}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             Content|               clean|\n",
      "+--------------------+--------------------+\n",
      "|At the MARS 2017 ...|At the MARS  tech...|\n",
      "|”I remember who t...|I remember who th...|\n",
      "|Billionaire Georg...|Billionaire Georg...|\n",
      "|On Friday’s Breit...|On Fridays Breitb...|\n",
      "|PIEDRAS NEGRAS, C...|PIEDRAS NEGRAS Co...|\n",
      "|Saturday at a tow...|Saturday at a tow...|\n",
      "|WASHINGTON  —   P...|WASHINGTON     Pr...|\n",
      "|White House press...|White House press...|\n",
      "|Anthem, the natio...|Anthem the nation...|\n",
      "|Sunday on ABC’s “...|Sunday on ABCs Th...|\n",
      "|Speaker Ryan’s pl...|Speaker Ryans pla...|\n",
      "|Conservative fire...|Conservative fire...|\n",
      "|Sunday on NBC’s “...|Sunday on NBCs Me...|\n",
      "|TEL AVIV  —   A J...|TEL AVIV     A Je...|\n",
      "|Although the NRA ...|Although the NRA ...|\n",
      "|Friday on Hugh He...|Friday on Hugh He...|\n",
      "|While the protest...|While the protest...|\n",
      "|At a Tuesday pres...|At a Tuesday pres...|\n",
      "|Players that prot...|Players that prot...|\n",
      "|  magazine Teen V...|  magazine Teen V...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_df = full_df.withColumn(\"clean\", regexp_replace('Content', reg ,\"\"))\n",
    "full_df.select('Content','clean').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               clean|              clean1|\n",
      "+--------------------+--------------------+\n",
      "|At the MARS  tech...|At the MARS tech ...|\n",
      "|I remember who th...|I remember who th...|\n",
      "|Billionaire Georg...|Billionaire Georg...|\n",
      "|On Fridays Breitb...|On Fridays Breitb...|\n",
      "|PIEDRAS NEGRAS Co...|PIEDRAS NEGRAS Co...|\n",
      "|Saturday at a tow...|Saturday at a tow...|\n",
      "|WASHINGTON     Pr...|WASHINGTON Presid...|\n",
      "|White House press...|White House press...|\n",
      "|Anthem the nation...|Anthem the nation...|\n",
      "|Sunday on ABCs Th...|Sunday on ABCs Th...|\n",
      "|Speaker Ryans pla...|Speaker Ryans pla...|\n",
      "|Conservative fire...|Conservative fire...|\n",
      "|Sunday on NBCs Me...|Sunday on NBCs Me...|\n",
      "|TEL AVIV     A Je...|TEL AVIV A Jewish...|\n",
      "|Although the NRA ...|Although the NRA ...|\n",
      "|Friday on Hugh He...|Friday on Hugh He...|\n",
      "|While the protest...|While the protest...|\n",
      "|At a Tuesday pres...|At a Tuesday pres...|\n",
      "|Players that prot...|Players that prot...|\n",
      "|  magazine Teen V...| magazine Teen Vo...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_df = full_df.withColumn(\"clean1\", regexp_replace('clean', reg1 ,\" \"))\n",
    "full_df.select('clean','clean1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización de los contenidos de las publicaciones\n",
    "Creacion de un dataframe con el contenido de la publicacion tokenizado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization=Tokenizer(inputCol='clean1',outputCol='tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df = tokenization.transform(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              clean1|              tokens|\n",
      "+--------------------+--------------------+\n",
      "|At the MARS tech ...|[at, the, mars, t...|\n",
      "|I remember who th...|[i, remember, who...|\n",
      "|Billionaire Georg...|[billionaire, geo...|\n",
      "|On Fridays Breitb...|[on, fridays, bre...|\n",
      "|PIEDRAS NEGRAS Co...|[piedras, negras,...|\n",
      "|Saturday at a tow...|[saturday, at, a,...|\n",
      "|WASHINGTON Presid...|[washington, pres...|\n",
      "|White House press...|[white, house, pr...|\n",
      "|Anthem the nation...|[anthem, the, nat...|\n",
      "|Sunday on ABCs Th...|[sunday, on, abcs...|\n",
      "|Speaker Ryans pla...|[speaker, ryans, ...|\n",
      "|Conservative fire...|[conservative, fi...|\n",
      "|Sunday on NBCs Me...|[sunday, on, nbcs...|\n",
      "|TEL AVIV A Jewish...|[tel, aviv, a, je...|\n",
      "|Although the NRA ...|[although, the, n...|\n",
      "|Friday on Hugh He...|[friday, on, hugh...|\n",
      "|While the protest...|[while, the, prot...|\n",
      "|At a Tuesday pres...|[at, a, tuesday, ...|\n",
      "|Players that prot...|[players, that, p...|\n",
      "| magazine Teen Vo...|[, magazine, teen...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_df.select('clean1','tokens').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminar stopWords\n",
    "Eliminación de stopWord en la columna de contenido de las publicaciones, token tales como \"I, and .or\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_df=stopword_removal.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              clean1|              tokens|      refined_tokens|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|At the MARS tech ...|[at, the, mars, t...|[mars, tech, conf...|\n",
      "|I remember who th...|[i, remember, who...|[remember, people...|\n",
      "|Billionaire Georg...|[billionaire, geo...|[billionaire, geo...|\n",
      "|On Fridays Breitb...|[on, fridays, bre...|[fridays, breitba...|\n",
      "|PIEDRAS NEGRAS Co...|[piedras, negras,...|[piedras, negras,...|\n",
      "|Saturday at a tow...|[saturday, at, a,...|[saturday, town, ...|\n",
      "|WASHINGTON Presid...|[washington, pres...|[washington, pres...|\n",
      "|White House press...|[white, house, pr...|[white, house, pr...|\n",
      "|Anthem the nation...|[anthem, the, nat...|[anthem, nations,...|\n",
      "|Sunday on ABCs Th...|[sunday, on, abcs...|[sunday, abcs, we...|\n",
      "|Speaker Ryans pla...|[speaker, ryans, ...|[speaker, ryans, ...|\n",
      "|Conservative fire...|[conservative, fi...|[conservative, fi...|\n",
      "|Sunday on NBCs Me...|[sunday, on, nbcs...|[sunday, nbcs, me...|\n",
      "|TEL AVIV A Jewish...|[tel, aviv, a, je...|[tel, aviv, jewis...|\n",
      "|Although the NRA ...|[although, the, n...|[although, nra, a...|\n",
      "|Friday on Hugh He...|[friday, on, hugh...|[friday, hugh, he...|\n",
      "|While the protest...|[while, the, prot...|[protest, movemen...|\n",
      "|At a Tuesday pres...|[at, a, tuesday, ...|[tuesday, press, ...|\n",
      "|Players that prot...|[players, that, p...|[players, protest...|\n",
      "| magazine Teen Vo...|[, magazine, teen...|[, magazine, teen...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_df.select('clean1','tokens','refined_tokens').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización del DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, Title: string, Publication: string, Author: string, Publication_date: string, Year: string, Month: string, URL: string, Content: string, clean: string, clean1: string, tokens: array<string>, refined_tokens: array<string>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_udf = udf(lambda s: len(s), IntegerType())\n",
    "refined_df = refined_df.withColumn(\"token_count\", len_udf(col('refined_tokens')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------+\n",
      "|              clean1|              tokens|      refined_tokens|token_count|\n",
      "+--------------------+--------------------+--------------------+-----------+\n",
      "|At the MARS tech ...|[at, the, mars, t...|[mars, tech, conf...|        233|\n",
      "|I remember who th...|[i, remember, who...|[remember, people...|         61|\n",
      "|Billionaire Georg...|[billionaire, geo...|[billionaire, geo...|        392|\n",
      "|On Fridays Breitb...|[on, fridays, bre...|[fridays, breitba...|        551|\n",
      "|PIEDRAS NEGRAS Co...|[piedras, negras,...|[piedras, negras,...|        192|\n",
      "|Saturday at a tow...|[saturday, at, a,...|[saturday, town, ...|         51|\n",
      "|WASHINGTON Presid...|[washington, pres...|[washington, pres...|        905|\n",
      "|White House press...|[white, house, pr...|[white, house, pr...|        175|\n",
      "|Anthem the nation...|[anthem, the, nat...|[anthem, nations,...|        174|\n",
      "|Sunday on ABCs Th...|[sunday, on, abcs...|[sunday, abcs, we...|        106|\n",
      "|Speaker Ryans pla...|[speaker, ryans, ...|[speaker, ryans, ...|        264|\n",
      "|Conservative fire...|[conservative, fi...|[conservative, fi...|        186|\n",
      "|Sunday on NBCs Me...|[sunday, on, nbcs...|[sunday, nbcs, me...|        113|\n",
      "|TEL AVIV A Jewish...|[tel, aviv, a, je...|[tel, aviv, jewis...|        207|\n",
      "|Although the NRA ...|[although, the, n...|[although, nra, a...|        260|\n",
      "|Friday on Hugh He...|[friday, on, hugh...|[friday, hugh, he...|         91|\n",
      "|While the protest...|[while, the, prot...|[protest, movemen...|        533|\n",
      "|At a Tuesday pres...|[at, a, tuesday, ...|[tuesday, press, ...|        254|\n",
      "|Players that prot...|[players, that, p...|[players, protest...|        266|\n",
      "| magazine Teen Vo...|[, magazine, teen...|[, magazine, teen...|        159|\n",
      "|Good morning Here...|[good, morning, h...|[good, morning, h...|        692|\n",
      "|During his regula...|[during, his, reg...|[regular, segment...|        256|\n",
      "|Ernie Els known a...|[ernie, els, know...|[ernie, els, know...|        662|\n",
      "|OLATHE Kan The Ja...|[olathe, kan, the...|[olathe, kan, jam...|        809|\n",
      "|The chairman of t...|[the, chairman, o...|[chairman, senate...|        249|\n",
      "|An imam involved ...|[an, imam, involv...|[imam, involved, ...|        367|\n",
      "|U S military offi...|[u, s, military, ...|[u, military, off...|        302|\n",
      "|A group of techno...|[a, group, of, te...|[group, technolog...|        202|\n",
      "|Kid Rocks name ha...|[kid, rocks, name...|[kid, rocks, name...|        129|\n",
      "|Senate Republican...|[senate, republic...|[senate, republic...|        372|\n",
      "|Today is Earth Ho...|[today, is, earth...|[today, earth, ho...|        608|\n",
      "|At the White Hous...|[at, the, white, ...|[white, house, th...|        109|\n",
      "|Spring The presid...|[spring, the, pre...|[spring, presiden...|        433|\n",
      "|In a recent inter...|[in, a, recent, i...|[recent, intervie...|        129|\n",
      "|The Jerusalem Pos...|[the, jerusalem, ...|[jerusalem, post,...|         77|\n",
      "|PARIS AP Deepenin...|[paris, ap, deepe...|[paris, ap, deepe...|        340|\n",
      "|Chief news execut...|[chief, news, exe...|[chief, news, exe...|        122|\n",
      "|Three robbers cut...|[three, robbers, ...|[three, robbers, ...|        146|\n",
      "|Sunday on MSNBCs ...|[sunday, on, msnb...|[sunday, msnbcs, ...|         68|\n",
      "|On Mondays broadc...|[on, mondays, bro...|[mondays, broadca...|        119|\n",
      "|On the Tuesday ed...|[on, the, tuesday...|[tuesday, edition...|        133|\n",
      "|Americans are bei...|[americans, are, ...|[americans, marke...|        726|\n",
      "|Police credit Ube...|[police, credit, ...|[police, credit, ...|        133|\n",
      "|As northern Flori...|[as, northern, fl...|[northern, florid...|        382|\n",
      "|A report claims t...|[a, report, claim...|[report, claims, ...|        324|\n",
      "|As Boston prepare...|[as, boston, prep...|[boston, prepares...|        462|\n",
      "|Good morning Here...|[good, morning, h...|[good, morning, h...|        628|\n",
      "|PHILADELPHIA Penn...|[philadelphia, pe...|[philadelphia, pe...|         57|\n",
      "|Hollywood and oth...|[hollywood, and, ...|[hollywood, media...|        667|\n",
      "|Josh Kraushaar at...|[josh, kraushaar,...|[josh, kraushaar,...|        212|\n",
      "+--------------------+--------------------+--------------------+-----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_df.select('clean1','tokens','refined_tokens','token_count').show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupación de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------+--------------------+----------------+------+-----+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+\n",
      "|   ID|               Title|   Publication|              Author|Publication_date|  Year|Month| URL|             Content|               clean|              clean1|              tokens|      refined_tokens|token_count|\n",
      "+-----+--------------------+--------------+--------------------+----------------+------+-----+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+\n",
      "|28828|Watch: Amazon Bos...|     Breitbart|         Nate Church|      2017-03-21|2017.0|  3.0|null|At the MARS 2017 ...|At the MARS  tech...|At the MARS tech ...|[at, the, mars, t...|[mars, tech, conf...|        233|\n",
      "|28837|Patriots Owner Ro...|     Breitbart|         Trent Baker|      2017-02-03|2017.0|  2.0|null|”I remember who t...|I remember who th...|I remember who th...|[i, remember, who...|[remember, people...|         61|\n",
      "|28972|Report: George So...|     Breitbart|         Aaron Klein|      2017-01-23|2017.0|  1.0|null|Billionaire Georg...|Billionaire Georg...|Billionaire Georg...|[billionaire, geo...|[billionaire, geo...|        392|\n",
      "|29249|Peter Schweizer: ...|     Breitbart|        John Hayward|      2017-05-26|2017.0|  5.0|null|On Friday’s Breit...|On Fridays Breitb...|On Fridays Breitb...|[on, fridays, bre...|[fridays, breitba...|        551|\n",
      "|29344|Mexican Border St...|     Breitbart|   Cartel Chronicles|      2017-02-19|2017.0|  2.0|null|PIEDRAS NEGRAS, C...|PIEDRAS NEGRAS Co...|PIEDRAS NEGRAS Co...|[piedras, negras,...|[piedras, negras,...|        192|\n",
      "|29382|Maxine Waters: ’D...|     Breitbart|             Pam Key|      2017-05-14|2017.0|  5.0|null|Saturday at a tow...|Saturday at a tow...|Saturday at a tow...|[saturday, at, a,...|[saturday, town, ...|         51|\n",
      "|17410|Putin Led a Compl...|New York Times|Michael D. Shear ...|      2017-01-07|2017.0|  1.0|null|WASHINGTON  —   P...|WASHINGTON     Pr...|WASHINGTON Presid...|[washington, pres...|[washington, pres...|        905|\n",
      "|29751|White House: ’No ...|     Breitbart|          Edwin Mora|      2017-01-25|2017.0|  1.0|null|White House press...|White House press...|White House press...|[white, house, pr...|[white, house, pr...|        175|\n",
      "|29821|Biggest Insurance...|     Breitbart|          Sean Moran|      2017-03-10|2017.0|  3.0|null|Anthem, the natio...|Anthem the nation...|Anthem the nation...|[anthem, the, nat...|[anthem, nations,...|        174|\n",
      "|30004|Miller: We Have P...|     Breitbart|             Pam Key|      2017-02-12|2017.0|  2.0|null|Sunday on ABC’s “...|Sunday on ABCs Th...|Sunday on ABCs Th...|[sunday, on, abcs...|[sunday, abcs, we...|        106|\n",
      "|30038|Speaker Ryan’s Ob...|     Breitbart|          Sean Moran|      2017-03-10|2017.0|  3.0|null|Speaker Ryan’s pl...|Speaker Ryans pla...|Speaker Ryans pla...|[speaker, ryans, ...|[speaker, ryans, ...|        264|\n",
      "|30067|Ann Coulter Tweet...|     Breitbart|         Lucas Nolan|      2017-05-13|2017.0|  5.0|null|Conservative fire...|Conservative fire...|Conservative fire...|[conservative, fi...|[conservative, fi...|        186|\n",
      "|30335|Conway to Chuck T...|     Breitbart|             Pam Key|      2017-01-22|2017.0|  1.0|null|Sunday on NBC’s “...|Sunday on NBCs Me...|Sunday on NBCs Me...|[sunday, on, nbcs...|[sunday, nbcs, me...|        113|\n",
      "|30431|WATCH: Vandals Tw...|     Breitbart|       Deborah Danan|      2017-01-01|2017.0|  1.0|null|TEL AVIV  —   A J...|TEL AVIV     A Je...|TEL AVIV A Jewish...|[tel, aviv, a, je...|[tel, aviv, jewis...|        207|\n",
      "|30468|Politico Magazine...|     Breitbart|         AWR Hawkins|      2017-04-30|2017.0|  4.0|null|Although the NRA ...|Although the NRA ...|Although the NRA ...|[although, the, n...|[although, nra, a...|        260|\n",
      "|30502|HHS Sec Price: Ex...|     Breitbart|           Jeff Poor|      2017-05-12|2017.0|  5.0|null|Friday on Hugh He...|Friday on Hugh He...|Friday on Hugh He...|[friday, on, hugh...|[friday, hugh, he...|         91|\n",
      "|30697| Professional Act...|     Breitbart|         Aaron Klein|      2017-01-18|2017.0|  1.0|null|While the protest...|While the protest...|While the protest...|[while, the, prot...|[protest, movemen...|        533|\n",
      "|30851|Putin: Syria Chem...|     Breitbart|        John Hayward|      2017-04-11|2017.0|  4.0|null|At a Tuesday pres...|At a Tuesday pres...|At a Tuesday pres...|[at, a, tuesday, ...|[tuesday, press, ...|        254|\n",
      "|31542|Former Anthem-Pro...|     Breitbart|         Dylan Gwinn|      2017-04-16|2017.0|  4.0|null|Players that prot...|Players that prot...|Players that prot...|[players, that, p...|[players, protest...|        266|\n",
      "|31566|Teen Vogue Calls ...|     Breitbart|       Jerome Hudson|      2017-06-12|2017.0|  6.0|null|  magazine Teen V...|  magazine Teen V...| magazine Teen Vo...|[, magazine, teen...|[, magazine, teen...|        159|\n",
      "+-----+--------------------+--------------+--------------------+----------------+------+-----+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aux_df = refined_df\n",
    "aux_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2150.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 170.0 failed 4 times, most recent failure: Lost task 22.3 in stage 170.0 (TID 696, hdpjupyter.dis.eafit.edu.co, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$createTransformFunc$1: (string) => array<string>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2039)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2060)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2079)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2104)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n\tat org.apache.spark.ml.feature.CountVectorizer.fit(CountVectorizer.scala:176)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$createTransformFunc$1: (string) => array<string>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-6c896a2b2989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"refined_tokens\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rawFeatures\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcvmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2150.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 170.0 failed 4 times, most recent failure: Lost task 22.3 in stage 170.0 (TID 696, hdpjupyter.dis.eafit.edu.co, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$createTransformFunc$1: (string) => array<string>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2039)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2060)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2079)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2104)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n\tat org.apache.spark.ml.feature.CountVectorizer.fit(CountVectorizer.scala:176)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$createTransformFunc$1: (string) => array<string>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"refined_tokens\", outputCol=\"rawFeatures\" )\n",
    "cvmodel = cv.fit(aux_df.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HashingTF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-ec26e9140466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maux_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrefined_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhashingTF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHashingTF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"refined_tokens\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rawFeatures\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumFeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfeaturizedData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashingTF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HashingTF' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
